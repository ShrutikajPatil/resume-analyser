[
  {
    "id": 1,
    "resumeText": "\n\n                                                                            Shrutika Patil  \nshrutika.j.patil2021@gmail.com                             +919172765891                              Mumbai, MH \n     \nPROFESSIONAL SUMMARY \n \nData Engineer with 3.4 years of experienced in designing, developing and managing robust data pipeline \nand architectures using technology like Python, Apache Spark, AWS services, SQL. Adept at ETL \nprocesses, data modelling and data warehousing solutions with hands-on expertise in cloud platform \n(Azure, AWS, GCP). Skilled on optimizing data workflows for performance, scalability and reliability while \nensuring data security and compliance. Strong problem-solving abilities with a focus on delivering data \nsolutions that support business intelligence and analytics. \n \nTECHNICAL SKILLS \n \nLanguagesSQL ,Python : \nAirflowPySpark, Pandas, : Frameworks/Libraries \n, Blob SynapseADF, Databricks, ( zure, A)3, Lambda, Redshift, Athena, SNS etc.Glue, SAWS (: Cloud\n)sage, Linked ServiceStor \nSnowflake tgreSQL,SQL Server, Pos: Databases \nCI/CD pipelines, Jenkins, Docker Git,: DevOps \n, Apache AtlasleauPower BI, Tab Tools: \n \nWORK EXPERIENCE  \n \nData Engineer at Wipro, Pune                                                                              April, 2023 – July, 2025 \n \n• Designed and developed scalable ETL pipelines using PySpark and SQL on AWS Glue, ensuring high-\nperformance data processing and optimized resource utilization. \nto unify data from multiple sources (ERP, CRM, and APIs) into a single analytical data  ETL pipelinesBuilt • \nlake, enabling seamless data access across teams. \n• Implement data validation, cleansing and transformation processes and design and manage data \nwarehouses, lakes and distributes systems. \n.GitHub Actions for data pipeline deployment using automated CI/CD pipelines detnemeImpl•  \n dataand implemented schema enforcement to ensure  Applied Data Validation and Testing Frameworks• \nacross all stages. quality, integrity, and reliability \n, including analysts, data scientists, and cloud architects, to functional teams-Collaborated with cross •\n.supporting business intelligence and ML workloads data models and governance frameworksdesign  \n \nData Engineer at Tech Mahindra, Pune                                                             April, 2022 – March, 2023 \n \n• Designed and implemented a cloud-native data platform on Azure to consolidate call logs, transcripts, and \ncustomer data into a unified data lake, ensuring reliable, scalable, and compliant data processing. \nfor incremental ETL and schema validation, enabling  Gold)–Silver–Built a Medallion architecture (Bronze •\ntime data refreshes for analytics and reporting.-near real \nfor data cleansing, deduplication, and  Engineered data pipelines using PySpark and Delta Lake •\nenrichment, ensuring high data quality and integrity across all layers. \nto deliver a  functional teams (data analysts, ML engineers, BI developers)-Collaborated with cross• \nready data foundation for Power BI dashboards and ML models.-production \n, improving SLA compliance and enabling Reduced report generation latency from 48 hours to 2 hours •\nmaking.-driven decision-data \nto support predictive churn and customer experience models,  old layergCreated ML feature store in the  •\naccelerating model training and deployment. \n \nEDUCATION  \n \nBachelor of Engineering | Information Technology July 2018- June 2022| Pune University | CGPA: 8.80 ",
    "jobDescription": "Strong proficiency in Python and Pyspark for data engineering tasks.\r\nHands-on experience with AWS Glue, Athena, EMR and Redshift.\r\nProficient in SQL with the ability to write complex queries for data analysis.\r\nBasic understanding of data modeling and data warehousing concepts.\r\nExcellent problem-solving and troubleshooting skills.\r\nProven track record of working in a collaborative team environment.\r\nExperience with version control systems and best practices.",
    "score": 85,
    "explanation": "The resume demonstrates strong proficiency in Python and PySpark, along with hands-on experience with AWS Glue and Redshift, closely aligning with the job requirements. However, there is a slight gap in explicitly mentioning experience with Athena and EMR, as well as a more detailed emphasis on collaborative work and version control practices.",
    "recommendations": [
      "Add specific experience with AWS Athena and EMR to showcase comprehensive AWS expertise.",
      "Highlight collaborative projects or teamwork experiences to align with the job's emphasis on teamwork.",
      "Include more details about version control systems used, particularly Git, to demonstrate adherence to best practices.",
      "Clarify understanding of data modeling and data warehousing concepts with specific examples or projects.",
      "Consider reformatting the technical skills section for better readability and emphasis on relevant tools."
    ],
    "fileName": "Shrutika.j.Patil_Resume.pdf",
    "timestamp": "2026-01-16T15:31:59.358Z"
  }
]